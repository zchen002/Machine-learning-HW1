{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Problem3.ipynb","version":"0.3.2","provenance":[{"file_id":"1UL6IlH0ACowMQX-RFeOGw73rusMi4iWK","timestamp":1551154346502}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"VefHvrfeRG_V","colab_type":"text"},"cell_type":"markdown","source":["# Softmax classification and categorical cross entropy loss"]},{"metadata":{"id":"KrYQtZBNmudf","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","from sklearn.utils import shuffle"],"execution_count":0,"outputs":[]},{"metadata":{"id":"235_uAfJnNn4","colab_type":"code","outputId":"071f700b-7dd8-4661-b486-050929286c78","executionInfo":{"status":"ok","timestamp":1551206094858,"user_tz":300,"elapsed":4994,"user":{"displayName":"Zhefu Cheng","photoUrl":"","userId":"18247105643606026618"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["# Load MNIST\n","from keras.datasets import mnist\n","\n","(train_images_original, train_labels_original), (test_images_original, test_labels_original) = mnist.load_data()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n","11493376/11490434 [==============================] - 2s 0us/step\n"],"name":"stdout"}]},{"metadata":{"id":"nCiNQYT5niql","colab_type":"code","outputId":"6de3baf8-bab2-432d-f220-50c0a20b0263","executionInfo":{"status":"ok","timestamp":1551206100741,"user_tz":300,"elapsed":683,"user":{"displayName":"Zhefu Cheng","photoUrl":"","userId":"18247105643606026618"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"cell_type":"code","source":["print(train_images_original.shape)\n","print(train_labels_original.shape)\n","print(test_images_original.shape)\n","print(test_labels_original.shape)\n","print(train_labels_original[0])\n","print(train_labels_original[10])\n","print(train_labels_original[21])\n","print(train_labels_original[34])\n","print(test_labels_original[0])\n","print(test_labels_original[10])\n","#print(train_images_original[0])"],"execution_count":3,"outputs":[{"output_type":"stream","text":["(60000, 28, 28)\n","(60000,)\n","(10000, 28, 28)\n","(10000,)\n","5\n","3\n","0\n","0\n","7\n","0\n"],"name":"stdout"}]},{"metadata":{"id":"d2iz7enuJ_jt","colab_type":"text"},"cell_type":"markdown","source":["# Data preprocessing"]},{"metadata":{"id":"4w7cOemCwh9a","colab_type":"code","outputId":"1869d452-ef02-41a4-d302-769b79eacb23","executionInfo":{"status":"ok","timestamp":1551206130490,"user_tz":300,"elapsed":960,"user":{"displayName":"Zhefu Cheng","photoUrl":"","userId":"18247105643606026618"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"cell_type":"code","source":["train_images_flat = train_images_original.reshape((60000, 28 * 28)).T\n","test_images_flat = test_images_original.reshape((10000, 28 * 28)).T\n","# Normalize data\n","train_images = train_images_flat / 255.\n","test_images = test_images_flat / 255.\n","train_labels = train_labels_original.reshape((train_labels_original.shape[0], 1)).T\n","test_labels = test_labels_original.reshape((test_labels_original.shape[0], 1)).T\n","\n","print(train_images.shape)\n","print(test_images.shape)\n","print(train_labels.shape)\n","print(test_labels.shape)\n","print(train_labels)\n","print(train_labels[0])\n","print(train_labels[0][0])\n","print(train_labels[0][1])\n","print(train_labels[0][2])\n","print(train_labels[0, 2])\n","print(train_labels[0][10])\n","print(train_labels[0][21])\n","print(train_labels[0][34])\n","print(train_labels[0][59997])\n","print(train_labels[0][59998])\n","print(train_labels[0][59999])\n","print(test_labels[0][0])\n","print(test_labels[0][1])\n","print(test_labels[0][2])\n","print(test_labels[0][10])\n","#print(train_images[0])"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(784, 60000)\n","(784, 10000)\n","(1, 60000)\n","(1, 10000)\n","[[5 0 4 ... 5 6 8]]\n","[5 0 4 ... 5 6 8]\n","5\n","0\n","4\n","4\n","3\n","0\n","0\n","5\n","6\n","8\n","7\n","2\n","1\n","0\n"],"name":"stdout"}]},{"metadata":{"id":"Pyxg_aNMGQ4S","colab_type":"text"},"cell_type":"markdown","source":["**One-hot encoded labels**"]},{"metadata":{"id":"uRkyoqTeAyHt","colab_type":"code","outputId":"5cb2c8ac-5b40-4a22-f2cf-ce783bb6c217","executionInfo":{"status":"ok","timestamp":1551206363696,"user_tz":300,"elapsed":688,"user":{"displayName":"Zhefu Cheng","photoUrl":"","userId":"18247105643606026618"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"cell_type":"code","source":["one_hot_train_labels = np.zeros(shape=(10, train_labels.shape[1]), dtype = int)\n","print(one_hot_train_labels.shape)\n","print(one_hot_train_labels[:, 0])\n","print(one_hot_train_labels[:, 1])\n","print(one_hot_train_labels[:, 2])\n","print(one_hot_train_labels[:, 10])\n","print()\n","\n","for i in range(train_labels.shape[1]):  \n","  one_hot_train_labels[train_labels[0, i], i] = 1\n","  \n","print(one_hot_train_labels[:, 0])\n","print(one_hot_train_labels[:, 1])\n","print(one_hot_train_labels[:, 2])\n","print(one_hot_train_labels[:, 10])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["(10, 60000)\n","[0 0 0 0 0 0 0 0 0 0]\n","[0 0 0 0 0 0 0 0 0 0]\n","[0 0 0 0 0 0 0 0 0 0]\n","[0 0 0 0 0 0 0 0 0 0]\n","\n","[0 0 0 0 0 1 0 0 0 0]\n","[1 0 0 0 0 0 0 0 0 0]\n","[0 0 0 0 1 0 0 0 0 0]\n","[0 0 0 1 0 0 0 0 0 0]\n"],"name":"stdout"}]},{"metadata":{"id":"MGkgwb1BKgma","colab_type":"text"},"cell_type":"markdown","source":["# Softmax classification model"]},{"metadata":{"id":"zeHpe8JW-Cgv","colab_type":"code","colab":{}},"cell_type":"code","source":["def stable_softmax(X):\n","    exps = np.exp(X - np.max(X))\n","    \n","    return exps / exps.sum(axis=1, keepdims=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XlvenHdTMUi5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"9688a037-2d6b-4409-f539-df748c745086","executionInfo":{"status":"ok","timestamp":1551206582548,"user_tz":300,"elapsed":601,"user":{"displayName":"Zhefu Cheng","photoUrl":"","userId":"18247105643606026618"}}},"cell_type":"code","source":["nums = np.array([[1,2,3], [4,5,6], [7,9,10], [8,11,14], [12,14,16]])  \n","print(nums.shape)\n","print(stable_softmax(nums))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(5, 3)\n","[[0.09003057 0.24472847 0.66524096]\n"," [0.09003057 0.24472847 0.66524096]\n"," [0.03511903 0.25949646 0.70538451]\n"," [0.00235563 0.04731416 0.95033021]\n"," [0.01587624 0.11731043 0.86681333]]\n"],"name":"stdout"}]},{"metadata":{"id":"tMQpzyH-FrTY","colab_type":"code","colab":{}},"cell_type":"code","source":["def initialize_params(dim):\n","\n","    # dim -- number of parameters\n","    \n","    w = np.zeros((dim, 10))\n","    b = 0.0\n","\n","    assert(w.shape == (dim, 10))\n","    assert(isinstance(b, float) or isinstance(b, int))\n","    \n","    return w, b"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f1qmp8f0Of3e","colab_type":"text"},"cell_type":"markdown","source":["**Categorical cross entropy loss**"]},{"metadata":{"id":"c9hkxXlvF7zf","colab_type":"code","colab":{}},"cell_type":"code","source":["def propagate(w, b, X, Y):\n","    \n","    # cost -- cross entropy loss\n","    # dw -- gradient of the loss with respect to w, thus same shape as w\n","    # db -- gradient of the loss with respect to b, thus same shape as b\n","    \n","    \n","    m = X.shape[1]\n","\n","    # 1. FORWARD PROPAGATION\n","\n","    A = (stable_softmax((np.dot(w.T, X) + b).T)).T # Apply softmax\n","    #print(A.shape)\n","    #print(Y.shape)\n","    \n","    # Y is one-hot encoded\n","    cost = 1/m * np.sum(-Y * np.log(A)) # cross entropy loss\n","\n","    # 2. BACKWARD PROPAGATION\n","    # Compute gradients of loss function\n","    dw = 1/m * np.dot(X, (A - Y).T)\n","    db = 1/m * np.sum(A - Y)\n","\n","    #print(dw.shape)\n","    #print(w.shape) # dw.shape = w.shape \n","    assert(dw.shape == w.shape) #telling the program to test that condition, and trigger an error if the condition is false\n","    assert(db.dtype == float)\n","    cost = np.squeeze(cost)\n","    assert(cost.shape == ())\n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    \n","    return grads, cost"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gKdUv0TkOu8W","colab_type":"text"},"cell_type":"markdown","source":["**Mini-batch gradient descent**"]},{"metadata":{"id":"oYkQXUxgGOtO","colab_type":"code","colab":{}},"cell_type":"code","source":["def optimize(w, b, X, Y, num_iters, learning_rate, batch_size, print_cost=False):\n","    \n","    # X, Y: training data and training labels\n","    \n","    costs = []\n","    #print(X.shape)\n","    #print(Y.shape)\n","    \n","    for iter in range(num_iters):\n","        # Randomize training data\n","        X, Y = shuffle(X.T, Y.T)\n","        minibatch_size = batch_size\n","        #if iter == 500:\n","          #print(X.shape)\n","          #print(Y.shape)\n","          \n","        # mini batch  \n","        for i in range(0, X.shape[0], minibatch_size):   \n","            X_mini = X[i:i + minibatch_size]\n","            Y_mini = Y[i:i + minibatch_size]\n","        #if iter == 500:\n","          #print(X_mini.shape)\n","          #print(Y_mini.shape)\n"," \n","        grads, cost = propagate(w, b, X_mini.T, Y_mini.T)                             \n","        \n","       \n","        # Retrieve gradients\n","        dw = grads[\"dw\"]\n","        db = grads[\"db\"]\n","        \n","        # Update w, b using gradient descent \n","        w = w - learning_rate * dw                    \n","        b = b - learning_rate * db\n","        \n","        X, Y = X.T, Y.T\n","        #if iter == 500:\n","          #print(X.shape)\n","          #print(Y.shape)\n","          \n","        # Record the costs\n","        if iter % 100 == 0:\n","            costs.append(cost)\n","        \n","        # Print the cost every 200 training examples\n","        if print_cost and (iter % 200 == 0 or iter == num_iters - 1):\n","            print (\"Cost after iteration %i: %f\" % (iter, cost))\n","    \n","    params = {\"w\": w,\n","              \"b\": b}\n","    \n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    \n","    return params, grads, costs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YN5ipobTGhIG","colab_type":"code","colab":{}},"cell_type":"code","source":["def predict(w, b, X):\n","    \n","    m = X.shape[1]\n","    \n","    Y_pred = []\n","    \n","    # Apply w and b that are learned through training\n","    A = stable_softmax((np.dot(w.T, X) + b).T) \n","    #print(A.shape)\n","    \n","    for i in range(A.shape[0]):\n","      Y_pred.append(np.argmax(A[i])) # Convert one-hot encoded labels to original labels (0-9)\n","    \n","    Y_pred = np.asarray(Y_pred)\n","    Y_pred = Y_pred.reshape((1, Y_pred.shape[0]))\n","    #print(Y_pred.shape)\n","    #print(Y_pred)\n","               \n","    assert(Y_pred.shape == (1, m))\n","    \n","    return Y_pred"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WdX3duxNGxhq","colab_type":"code","colab":{}},"cell_type":"code","source":["def model(X_train, Y_train, X_test, Y_test, num_iters=2000, learning_rate=0.005, batch_size=32, print_cost=False):\n","        \n","    #Initialize parameters (with zeros)\n","    w, b = initialize_params(X_train.shape[0])                            \n","\n","    # Gradient descent\n","    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iters, learning_rate, batch_size, print_cost)   \n","    \n","    # Retrieve trained w and b\n","    w = parameters[\"w\"]\n","    b = parameters[\"b\"]\n","    \n","    # Predict on test/train set\n","    Y_pred_test = predict(w, b, X_test)\n","    Y_pred_train = predict(w, b, X_train)\n","    \n","    count1 = 0\n","    count2 = 0\n","    # Compute training accuracy \n","    for i in range(Y_train.shape[1]):\n","      if(Y_pred_train[0,i] == train_labels[0,i]): # not Y_train[0,i] (one-hot encoded), but the original labels\n","        count1 += 1\n","    train_accuracy = count1 / Y_train.shape[1]\n","    \n","    # Compute test accuracy\n","    for j in range(Y_test.shape[1]):\n","      if(Y_pred_test[0,j] == Y_test[0,j]):\n","        count2 += 1\n","    test_accuracy = count2 / Y_test.shape[1]\n","    \n","    # Print train/test accuracy\n","    print(\"\")\n","    print(\"train accuracy: {} %\".format(train_accuracy * 100))\n","    print(\"test accuracy: {} %\".format(test_accuracy * 100))\n","    #print(w)\n","    #print(b)\n","\n","    # d -- dictionary storing information about the model\n","    d = {\"costs\": costs,\n","         \"Y_pred_test\": Y_pred_test, \n","         \"Y_pred_train\" : Y_pred_train, \n","         \"w\" : w, \n","         \"b\" : b,\n","         \"learning_rate\" : learning_rate,\n","         \"num_iters\": num_iters}\n","    \n","    return d"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ruR7usK9QHmB","colab_type":"text"},"cell_type":"markdown","source":["# MNIST digit classification"]},{"metadata":{"id":"ZKD3px11txjA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":255},"outputId":"22e1d58c-433f-4468-94f3-dabdb220a223","executionInfo":{"status":"ok","timestamp":1551209415049,"user_tz":300,"elapsed":272426,"user":{"displayName":"Zhefu Cheng","photoUrl":"","userId":"18247105643606026618"}}},"cell_type":"code","source":["classifier = model(train_images, one_hot_train_labels, test_images, test_labels, num_iters=2000, learning_rate=0.1, batch_size=128, print_cost=True)\n","w = classifier[\"w\"]\n","b = classifier[\"b\"]"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Cost after iteration 0: 2.302585\n","Cost after iteration 200: 0.397438\n","Cost after iteration 400: 0.384394\n","Cost after iteration 600: 0.271052\n","Cost after iteration 800: 0.374602\n","Cost after iteration 1000: 0.474772\n","Cost after iteration 1200: 0.480486\n","Cost after iteration 1400: 0.598959\n","Cost after iteration 1600: 0.313791\n","Cost after iteration 1800: 0.323261\n","Cost after iteration 1999: 0.231946\n","\n","train accuracy: 91.04166666666667 %\n","test accuracy: 91.47 %\n"],"name":"stdout"}]},{"metadata":{"id":"YML04xSoAlmu","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}